import json
import time
from rich.console import Console
# 1. æ­£ç¡®å¯¼å…¥ Prompt æ¥å£
from core.prompts import get_batch_translation_prompt
# 2. åŠ¨æ€å¯¼å…¥ LLM è°ƒç”¨å‡½æ•°
try:
    from core.ask_gpt import ask_gpt
except ImportError:
    from core.utils.ask_gpt import ask_gpt

console = Console()

def translate_batch_lines(lines, context_before, context_after, chunk_index=0):
    """
    å¯¹ä¸€ç»„å­—å¹•è¡Œè¿›è¡Œ Batch ç¿»è¯‘ (Version C + è‡ªåŠ¨é™çº§)
    """
    # ==========================
    # ç­–ç•¥ 1: å°è¯•æ‰¹é‡ç¿»è¯‘ (Batch Mode)
    # ==========================
    prompt = get_batch_translation_prompt(lines, context_before, context_after)
    
    # å®šä¹‰éªŒè¯å‡½æ•°ï¼šæ£€æŸ¥è¡Œæ•°æ˜¯å¦ä¸€è‡´
    def valid_length(response_data):
        if 'translation' not in response_data:
            return {"status": "error", "message": "Missing 'translation' key"}
        if not isinstance(response_data['translation'], list):
            return {"status": "error", "message": "'translation' must be a list"}
        if len(response_data['translation']) != len(lines):
            return {
                "status": "error", 
                "message": f"Length mismatch: Input {len(lines)} vs Output {len(response_data['translation'])}"
            }
        return {"status": "success", "message": "Valid"}

    try:
        # è°ƒç”¨ LLMï¼Œå°è¯• 2 æ¬¡ (å‡å°‘é‡è¯•æ¬¡æ•°ï¼Œé¿å…è§¦å‘ Rate Limit)
        # å¦‚æœ Batch å¤±è´¥ï¼Œå°½å¿«é™çº§åˆ°ä¸²è¡Œï¼Œä¸è¦æ­»ç£•
        response = ask_gpt(
            prompt, 
            resp_type='json', 
            valid_def=valid_length, 
            log_title=f'batch_trans_{chunk_index}'
        )
        return response['translation']

    except Exception as e:
        # å¦‚æœ Batch æ¨¡å¼å½»åº•å¤±è´¥ï¼ˆé€šå¸¸æ˜¯å› ä¸ºæ¨¡å‹éè¦åˆå¹¶è¡Œï¼‰ï¼Œè¿›å…¥é™çº§æ¨¡å¼
        console.print(f"[bold red]âŒ Chunk {chunk_index} Batch failed: {e}[/bold red]")
        console.print(f"[yellow]ğŸ”„ Falling back to Serial Translation (Line-by-Line) for Chunk {chunk_index}...[/yellow]")

    # ==========================
    # ç­–ç•¥ 2: é™çº§ä¸ºé€è¡Œç¿»è¯‘ (Serial Fallback)
    # ==========================
    # æ—¢ç„¶æ‰¹é‡å¯¹é½å¤±è´¥ï¼Œæˆ‘ä»¬å°±ä¸€è¡Œä¸€è¡Œç¿»ï¼Œè™½ç„¶æ…¢ï¼Œä½†ç»å¯¹ç¨³ã€‚
    
    fallback_result = []
    
    for i, line in enumerate(lines):
        # æ„é€ è¿™ä¸€è¡Œçš„ä¸“å±ä¸Šä¸‹æ–‡
        # ä¸Šæ–‡ = åŸå§‹ä¸Šæ–‡ + æœ¬ Batch ä¸­å·²ç»åœ¨è¿™ä¸€è¡Œä¹‹å‰çš„è¡Œ
        current_context_before = context_before + lines[:i]
        # ä¸‹æ–‡ = æœ¬ Batch ä¸­è¿™ä¸€è¡Œä¹‹åçš„è¡Œ + åŸå§‹ä¸‹æ–‡
        current_context_after = lines[i+1:] + context_after
        
        # æ„é€ ä¸€ä¸ªåªæœ‰ 1 è¡Œçš„ Batch Prompt (è¿™å°±å˜æˆäº†å•è¡Œç¿»è¯‘)
        single_prompt = get_batch_translation_prompt([line], current_context_before, current_context_after)
        
        try:
            # è¿™é‡Œçš„ valid_def ä¾ç„¶æ£€æŸ¥é•¿åº¦ï¼ˆå¿…é¡»æ˜¯1ï¼‰
            single_resp = ask_gpt(
                single_prompt,
                resp_type='json',
                valid_def=lambda r: {"status": "success", "message": ""} if len(r.get('translation', [])) == 1 else {"status": "error", "message": "1:1 check failed"},
                log_title=f'serial_{chunk_index}_{i}'
            )
            fallback_result.extend(single_resp['translation'])
        except Exception as e_single:
            console.print(f"[red]âŒ Line {i} failed in serial mode: {e_single}. Using source text.[/red]")
            # æœ€åçš„æœ€åï¼Œå¦‚æœå•è¡Œä¹Ÿç¿»ä¸å‡ºæ¥ï¼ˆæç½•è§ï¼‰ï¼Œæ‰ç”¨åŸæ–‡å…œåº•
            fallback_result.append(line)
            
    return fallback_result